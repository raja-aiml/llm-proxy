[project]
name = "llm-wrapper"
version = "0.1.0"
description = "OpenAI-compatible wrapper for LLM APIs with async and sync support"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "fastapi>=0.104.0",
    "uvicorn>=0.23.2",
    "pydantic>=2.4.2",
    "requests>=2.31.0",
    "aiohttp>=3.8.6",
    "python-dotenv>=1.0.0",
    "click>=8.1.7",
    "openai>=1.1.1",
    "pyyaml>=6.0.2",
    "opentelemetry-instrumentation-fastapi>=0.54b0",
    "opentelemetry-api>=1.33.0",
    "opentelemetry-sdk>=1.33.0",
    "opentelemetry-instrumentation-requests>=0.54b0",
    "opentelemetry-instrumentation-httpx>=0.54b0",
    "opentelemetry-exporter-otlp>=1.33.0",
    "opentelemetry-exporter-prometheus<=1.12.0rc1",
    "rich>=14.0.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.4.3",
    "pytest-cov>=4.1.0",
    "pytest-asyncio>=0.21.1",
    "httpx>=0.25.1",
    "respx>=0.20.2",
    "rich>=13.6.0",
]

[pytest]
asyncio_mode = "auto"
asyncio_default_fixture_loop_scope = "function"

[project.scripts]
llm-cli = "llm_wrapper.client.cli:client"
llm-server = "llm_wrapper.server.main:run"

[build-system]
requires = ["setuptools>=68.0.0", "wheel"]
build-backend = "setuptools.build_meta"

[tool.setuptools.package-dir]
"" = "src"

[tool.setuptools.packages.find]
where = ["src"]
include = ["llm_wrapper", "llm_wrapper.*"]

[tool.pytest.ini_options]
testpaths = ["tests"]
asyncio_mode = "auto"

[tool.coverage.run]
source = ["src"]
omit = ["tests/*"]